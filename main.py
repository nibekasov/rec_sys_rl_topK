import os
import torch
import torch.nn as nn
import torch.backends.cudnn as cudnn
import torch_optimizer as optim
import numpy as np
import pandas as pd
import pickle
from tqdm.auto import tqdm
from time import gmtime, strftime
import argparse
import wandb  # ‚úÖ –î–æ–±–∞–≤–ª—è–µ–º Weights & Biases

from IPython.display import clear_output
import matplotlib.pyplot as plt

from utils.env import FrameEnv
from utils.plot import Plotter
from model import Critic, DiscreteActor, Beta, Reinforce
from reinforce import ChooseREINFORCE
from utils.data_utils import make_items_tensor 

try:
    cuda = torch.device('cuda')
except Exception:
    print("We dont use CUDA rn")

import os

# –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, –≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è `main.py`
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –ø—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º –∏ –º–æ–¥–µ–ª—è–º
DATA_DIR = os.path.join(BASE_DIR, "data", "ml-1m")
MODEL_DIR = os.path.join(BASE_DIR, "models")


# üîπ –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
parser = argparse.ArgumentParser()
parser.add_argument("--lr", type=float, default=0.001, help="learning rate")
parser.add_argument("--dropout", type=float, default=0.5, help="dropout rate")
parser.add_argument("--frame_size", type=int, default=10, help="frame size for training")
parser.add_argument("--batch_size", type=int, default=10, help="batch size for training")
parser.add_argument("--epochs", type=int, default=20, help="training epochs")
parser.add_argument("--top_k", type=int, default=10, help="compute metrics@top_k")
parser.add_argument("--embedding_dim", type=int, default=32, help="dimension of embedding")
parser.add_argument("--policy_input_dim", type=int, default=1024, help="input dimension for policy/value net")
parser.add_argument("--policy_hidden_dim", type=int, default=4096, help="hidden dimension for policy/value net")
parser.add_argument("--data_path", type=str, default=DATA_DIR)
parser.add_argument("--model_path", type=str, default=MODEL_DIR)
parser.add_argument("--init_weight", type=float, default=0.01, help="initial weight value")
parser.add_argument("--plot_every", type=int, default=100, help="how many steps to plot the result")
parser.add_argument("--disable-cuda", action="store_true", help="Disable CUDA")
args = parser.parse_args()

# üîπ –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (GPU –∏–ª–∏ CPU)
args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {args.device}")  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è


# üîπ –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
args.embedding_path = os.path.join(args.data_path, 'ml20_pca128.pkl')
args.rating_path = os.path.join(args.data_path, 'ratings.csv')

cudnn.benchmark = True


if __name__ == '__main__':
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Weights & Biases
    wandb.init(project="reinforce-topK", config=vars(args))

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    movie_embeddings_key_dict = pickle.load(open(args.embedding_path, "rb"))

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ —Ç–µ–Ω–∑–æ—Ä—ã
    embeddings, key_to_id, id_to_key = make_items_tensor(movie_embeddings_key_dict)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º num_items
    num_items = len(embeddings)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ
    env = FrameEnv(args.embedding_path, args.rating_path, num_items, frame_size=10, 
                   batch_size=25, num_workers=1, test_size=0.05)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ç–µ–π –∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
    beta_net = Beta(num_items).to(args.device)
    value_net = Critic(args.policy_input_dim, args.policy_hidden_dim, num_items, 
                       args.dropout, args.init_weight).to(args.device)
    policy_net = DiscreteActor(args.policy_input_dim, args.policy_hidden_dim, num_items).to(args.device)

    policy_net.action_source = {'pi': 'beta', 'beta': 'beta'}

    reinforce = Reinforce(policy_net, value_net).to(args.device)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–µ—Ç–æ–¥–∞ –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏—è
    def select_action_corr(state, action, K, writer, step, **kwargs):
        return reinforce.nets['policy_net']._select_action_with_TopK_correction(
            state, beta_net.forward, action, K=K, writer=wandb, step=step
        )

    reinforce.nets['policy_net'].select_action = select_action_corr
    reinforce.params['reinforce'] = ChooseREINFORCE(ChooseREINFORCE.reinforce_with_TopK_correction)
    reinforce.params['K'] = 10

    # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç Plotter
    plotter = Plotter(reinforce.loss_layout, [['value', 'policy']])

    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    for epoch in range(args.epochs):
        for batch in tqdm(env.train_dataloader):
            loss = reinforce.update(batch)  # –í—ã–∑–æ–≤ update
            if loss:
                plotter.log_losses(loss)  # ‚úÖ –í—ã–∑—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ —ç–∫–∑–µ–º–ø–ª—è—Ä `plotter`
                wandb.log({"loss": loss})

            if reinforce._step % args.plot_every == 0:
                clear_output(True)
                print('step', reinforce._step)
                plotter.plot_loss()
                wandb.log({"step": reinforce._step, "plot_loss": plotter.get_current_loss()})



    # ‚úÖ –ó–∞–∫—Ä—ã–≤–∞–µ–º `wandb` –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
    #wandb.finish()
